{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PokeGAN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4V54RmprVUe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
        "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
        "import torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\n",
        "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
        "from torch.utils.data import (\n",
        "    DataLoader,\n",
        ")  # Gives easier dataset managment and creates mini batches\n",
        "from torch.utils.tensorboard import SummaryWriter  # to print to tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8cU3MWlrc5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.0005\n",
        "batch_size = 64\n",
        "image_size = 64\n",
        "channels_img = 3\n",
        "channels_noise = 256\n",
        "\n",
        "\n",
        "# For how many channels Generator and Discriminator should use\n",
        "features_d = 16\n",
        "features_g = 16\n",
        "\n",
        "my_transforms = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,)),\n",
        "    ]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApSmdzDAH-hR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset= '/content/pokemon'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kQw29prJ-DI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "folder = datasets.ImageFolder(dataset,transform=my_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odPQXjPH7oK0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataloader = DataLoader(folder,batch_size=batch_size,shuffle = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEtIF9oXI-Uf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, channels_img, features_d):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            # N x channels_img x 64 x 64\n",
        "            nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # N x features_d x 32 x 32\n",
        "            nn.Conv2d(features_d, features_d * 2, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(features_d * 2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(\n",
        "                features_d * 2, features_d * 4, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(features_d * 4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Conv2d(\n",
        "                features_d * 4, features_d * 8, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(features_d * 8),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            # N x features_d*8 x 4 x 4\n",
        "            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=2, padding=0),\n",
        "            # N x 1 x 1 x 1\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orHafXi3KbWI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, channels_noise, channels_img, features_g):\n",
        "        super(Generator, self).__init__()\n",
        "\n",
        "        self.net = nn.Sequential(\n",
        "            # N x channels_noise x 1 x 1\n",
        "            nn.ConvTranspose2d(\n",
        "                channels_noise, features_g * 16, kernel_size=4, stride=1, padding=0\n",
        "            ),\n",
        "            nn.BatchNorm2d(features_g * 16),\n",
        "            nn.ReLU(),\n",
        "            # N x features_g*16 x 4 x 4\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 16, features_g * 8, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(features_g * 8),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 8, features_g * 4, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(features_g * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 4, features_g * 2, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            nn.BatchNorm2d(features_g * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(\n",
        "                features_g * 2, channels_img, kernel_size=4, stride=2, padding=1\n",
        "            ),\n",
        "            # N x channels_img x 64 x 64\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR39WudWKm6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG83HnWoKe0m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "netD = Discriminator(channels_img,features_d).to(device)\n",
        "netG= Generator(channels_noise,channels_img,features_g).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LntYx-n5Khh-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizerD = optim.Adam(netD.parameters(),lr = lr, betas=(0.5,0.999))\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(0.5, 0.999))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x62ZiRtiKtYn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "e6826e41-ffe0-45bb-ad90-50e8fed223bf"
      },
      "source": [
        "netG.train()\n",
        "netD.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (net): Sequential(\n",
              "    (0): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2)\n",
              "    (2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (4): LeakyReLU(negative_slope=0.2)\n",
              "    (5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (6): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): LeakyReLU(negative_slope=0.2)\n",
              "    (8): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
              "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (10): LeakyReLU(negative_slope=0.2)\n",
              "    (11): Conv2d(128, 1, kernel_size=(4, 4), stride=(2, 2))\n",
              "    (12): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6P7fWuhKvTn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion =  nn.BCELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1UF4azBKxiY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "real_label  = 1\n",
        "fake_label = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-AalyanEKy-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fixed_noise = torch.randn(64,channels_noise,1,1).to(device)\n",
        "writer_real = SummaryWriter(f\"runs/GAN_MNIST/test_real\")\n",
        "writer_fake = SummaryWriter(f\"runs/GAN_MNIST/test_fake\")\n",
        "step = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fde6tVVLNrMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_epochs= 600"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "klf1bC2qK0eH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d8b1cdc3-bee5-4816-f187-fe243f09d0ac"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "  for batch_idx, (data,targets) in enumerate(dataloader):\n",
        "    data = data.to(device)\n",
        "    batch_size = data.shape[0]\n",
        "\n",
        "\n",
        "    netD.zero_grad()\n",
        "    label = (torch.ones(batch_size) * 0.9).to(device)\n",
        "    output = netD(data).reshape(-1)\n",
        "    lossD_real = criterion(output,label)\n",
        "    D_x= output.mean().item()\n",
        "\n",
        "    noise = torch.randn(batch_size,channels_noise,1,1).to(device)\n",
        "    fake = netG(noise)\n",
        "    label = (torch.ones(batch_size) * 0.1).to(device)\n",
        "    \n",
        "\n",
        "    output = netD(fake.detach()).reshape(-1)\n",
        "    lossD_fake = criterion(output, label)\n",
        "\n",
        "    lossD = lossD_real + lossD_fake\n",
        "    lossD.backward()\n",
        "    optimizerD.step()\n",
        "\n",
        "        ### Train Generator: max log(D(G(z)))\n",
        "    netG.zero_grad()\n",
        "    label = torch.ones(batch_size).to(device)\n",
        "    output = netD(fake).reshape(-1)\n",
        "    lossG = criterion(output, label)\n",
        "    lossG.backward()\n",
        "    optimizerG.step()\n",
        "\n",
        "        # Print losses ocassionally and print to tensorboard\n",
        "    if batch_idx % 100 == 0: \n",
        "\n",
        "\n",
        "      step += 1\n",
        "      print(\n",
        "                f\"Epoch [{epoch}/{num_epochs}] Batch {batch_idx}/{len(dataloader)} \\\n",
        "                  Loss D: {lossD:.4f}, loss G: {lossG:.4f} D(x): {D_x:.4f}\"\n",
        "            )\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "        fake = netG(fixed_noise)\n",
        "        img_grid_real = torchvision.utils.make_grid(data[:32], normalize=True)\n",
        "        img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
        "        writer_real.add_image(\n",
        "                    \"Mnist Real Images\", img_grid_real, global_step=step\n",
        "                )\n",
        "        writer_fake.add_image(\n",
        "                    \"Mnist Fake Images\", img_grid_fake, global_step=step\n",
        "                )\n",
        "\n",
        "      \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0/600] Batch 0/13                   Loss D: 1.2839, loss G: 1.3207 D(x): 0.4961\n",
            "Epoch [1/600] Batch 0/13                   Loss D: 0.9572, loss G: 3.7662 D(x): 0.7518\n",
            "Epoch [2/600] Batch 0/13                   Loss D: 0.7872, loss G: 3.6918 D(x): 0.7202\n",
            "Epoch [3/600] Batch 0/13                   Loss D: 1.2020, loss G: 5.3721 D(x): 0.8510\n",
            "Epoch [4/600] Batch 0/13                   Loss D: 0.8653, loss G: 3.6022 D(x): 0.6555\n",
            "Epoch [5/600] Batch 0/13                   Loss D: 0.8612, loss G: 2.9853 D(x): 0.6385\n",
            "Epoch [6/600] Batch 0/13                   Loss D: 0.7717, loss G: 3.9708 D(x): 0.8012\n",
            "Epoch [7/600] Batch 0/13                   Loss D: 0.7772, loss G: 3.4170 D(x): 0.9132\n",
            "Epoch [8/600] Batch 0/13                   Loss D: 1.2209, loss G: 2.1192 D(x): 0.6504\n",
            "Epoch [9/600] Batch 0/13                   Loss D: 1.0990, loss G: 1.6471 D(x): 0.5632\n",
            "Epoch [10/600] Batch 0/13                   Loss D: 1.1145, loss G: 1.2942 D(x): 0.5340\n",
            "Epoch [11/600] Batch 0/13                   Loss D: 1.0174, loss G: 1.4960 D(x): 0.6025\n",
            "Epoch [12/600] Batch 0/13                   Loss D: 1.3218, loss G: 1.8405 D(x): 0.5927\n",
            "Epoch [13/600] Batch 0/13                   Loss D: 1.2713, loss G: 1.9724 D(x): 0.7261\n",
            "Epoch [14/600] Batch 0/13                   Loss D: 1.1442, loss G: 1.5439 D(x): 0.7049\n",
            "Epoch [15/600] Batch 0/13                   Loss D: 1.0857, loss G: 2.2802 D(x): 0.7052\n",
            "Epoch [16/600] Batch 0/13                   Loss D: 1.1322, loss G: 1.4695 D(x): 0.5029\n",
            "Epoch [17/600] Batch 0/13                   Loss D: 1.6552, loss G: 0.6381 D(x): 0.2591\n",
            "Epoch [18/600] Batch 0/13                   Loss D: 1.1199, loss G: 1.7516 D(x): 0.6692\n",
            "Epoch [19/600] Batch 0/13                   Loss D: 1.1937, loss G: 2.3752 D(x): 0.6839\n",
            "Epoch [20/600] Batch 0/13                   Loss D: 1.2789, loss G: 1.0529 D(x): 0.3955\n",
            "Epoch [21/600] Batch 0/13                   Loss D: 0.9915, loss G: 2.3730 D(x): 0.6248\n",
            "Epoch [22/600] Batch 0/13                   Loss D: 0.7822, loss G: 2.2318 D(x): 0.8237\n",
            "Epoch [23/600] Batch 0/13                   Loss D: 0.8019, loss G: 2.4266 D(x): 0.7265\n",
            "Epoch [24/600] Batch 0/13                   Loss D: 0.7990, loss G: 2.6516 D(x): 0.7606\n",
            "Epoch [25/600] Batch 0/13                   Loss D: 0.9167, loss G: 2.2148 D(x): 0.7109\n",
            "Epoch [26/600] Batch 0/13                   Loss D: 1.0585, loss G: 1.4085 D(x): 0.6181\n",
            "Epoch [27/600] Batch 0/13                   Loss D: 1.7727, loss G: 3.1898 D(x): 0.8590\n",
            "Epoch [28/600] Batch 0/13                   Loss D: 1.3356, loss G: 1.0087 D(x): 0.4677\n",
            "Epoch [29/600] Batch 0/13                   Loss D: 1.1274, loss G: 1.5152 D(x): 0.5449\n",
            "Epoch [30/600] Batch 0/13                   Loss D: 1.1207, loss G: 1.9805 D(x): 0.7062\n",
            "Epoch [31/600] Batch 0/13                   Loss D: 1.1115, loss G: 1.3008 D(x): 0.5834\n",
            "Epoch [32/600] Batch 0/13                   Loss D: 1.1846, loss G: 2.3315 D(x): 0.7494\n",
            "Epoch [33/600] Batch 0/13                   Loss D: 1.3749, loss G: 1.1599 D(x): 0.4360\n",
            "Epoch [34/600] Batch 0/13                   Loss D: 1.4322, loss G: 3.0725 D(x): 0.7847\n",
            "Epoch [35/600] Batch 0/13                   Loss D: 1.0935, loss G: 1.5175 D(x): 0.5862\n",
            "Epoch [36/600] Batch 0/13                   Loss D: 1.1996, loss G: 1.0870 D(x): 0.4860\n",
            "Epoch [37/600] Batch 0/13                   Loss D: 1.1472, loss G: 1.2947 D(x): 0.5531\n",
            "Epoch [38/600] Batch 0/13                   Loss D: 0.9472, loss G: 1.8626 D(x): 0.6397\n",
            "Epoch [39/600] Batch 0/13                   Loss D: 1.0651, loss G: 1.1928 D(x): 0.5628\n",
            "Epoch [40/600] Batch 0/13                   Loss D: 0.9352, loss G: 1.9165 D(x): 0.7076\n",
            "Epoch [41/600] Batch 0/13                   Loss D: 1.1024, loss G: 1.6167 D(x): 0.6241\n",
            "Epoch [42/600] Batch 0/13                   Loss D: 1.2472, loss G: 0.8815 D(x): 0.4025\n",
            "Epoch [43/600] Batch 0/13                   Loss D: 0.8836, loss G: 1.9656 D(x): 0.7244\n",
            "Epoch [44/600] Batch 0/13                   Loss D: 0.9879, loss G: 2.3352 D(x): 0.7814\n",
            "Epoch [45/600] Batch 0/13                   Loss D: 1.0386, loss G: 1.7297 D(x): 0.6202\n",
            "Epoch [46/600] Batch 0/13                   Loss D: 1.0843, loss G: 1.5494 D(x): 0.6396\n",
            "Epoch [47/600] Batch 0/13                   Loss D: 1.0505, loss G: 1.2117 D(x): 0.5565\n",
            "Epoch [48/600] Batch 0/13                   Loss D: 1.1682, loss G: 2.6651 D(x): 0.8007\n",
            "Epoch [49/600] Batch 0/13                   Loss D: 1.0038, loss G: 2.2912 D(x): 0.7301\n",
            "Epoch [50/600] Batch 0/13                   Loss D: 1.5734, loss G: 3.9293 D(x): 0.8812\n",
            "Epoch [51/600] Batch 0/13                   Loss D: 0.9563, loss G: 1.7315 D(x): 0.6192\n",
            "Epoch [52/600] Batch 0/13                   Loss D: 0.8774, loss G: 1.9257 D(x): 0.6877\n",
            "Epoch [53/600] Batch 0/13                   Loss D: 1.6271, loss G: 0.8135 D(x): 0.2763\n",
            "Epoch [54/600] Batch 0/13                   Loss D: 1.2079, loss G: 1.3082 D(x): 0.5376\n",
            "Epoch [55/600] Batch 0/13                   Loss D: 1.1050, loss G: 1.4532 D(x): 0.4942\n",
            "Epoch [56/600] Batch 0/13                   Loss D: 1.0441, loss G: 1.4560 D(x): 0.6490\n",
            "Epoch [57/600] Batch 0/13                   Loss D: 0.9784, loss G: 1.8283 D(x): 0.6814\n",
            "Epoch [58/600] Batch 0/13                   Loss D: 1.1749, loss G: 1.8470 D(x): 0.7001\n",
            "Epoch [59/600] Batch 0/13                   Loss D: 1.1294, loss G: 1.8234 D(x): 0.6304\n",
            "Epoch [60/600] Batch 0/13                   Loss D: 1.0571, loss G: 1.1233 D(x): 0.5849\n",
            "Epoch [61/600] Batch 0/13                   Loss D: 1.0948, loss G: 1.7706 D(x): 0.6638\n",
            "Epoch [62/600] Batch 0/13                   Loss D: 1.1334, loss G: 1.2497 D(x): 0.5704\n",
            "Epoch [63/600] Batch 0/13                   Loss D: 1.1806, loss G: 0.8025 D(x): 0.4943\n",
            "Epoch [64/600] Batch 0/13                   Loss D: 1.0559, loss G: 1.7993 D(x): 0.6976\n",
            "Epoch [65/600] Batch 0/13                   Loss D: 1.0505, loss G: 2.1619 D(x): 0.7205\n",
            "Epoch [66/600] Batch 0/13                   Loss D: 1.1768, loss G: 0.9669 D(x): 0.4760\n",
            "Epoch [67/600] Batch 0/13                   Loss D: 1.1132, loss G: 2.3189 D(x): 0.7002\n",
            "Epoch [68/600] Batch 0/13                   Loss D: 1.1197, loss G: 1.6763 D(x): 0.6630\n",
            "Epoch [69/600] Batch 0/13                   Loss D: 0.9977, loss G: 1.5036 D(x): 0.5558\n",
            "Epoch [70/600] Batch 0/13                   Loss D: 1.0300, loss G: 1.7203 D(x): 0.7118\n",
            "Epoch [71/600] Batch 0/13                   Loss D: 1.0004, loss G: 1.6146 D(x): 0.6489\n",
            "Epoch [72/600] Batch 0/13                   Loss D: 1.0039, loss G: 1.6347 D(x): 0.6914\n",
            "Epoch [73/600] Batch 0/13                   Loss D: 1.1716, loss G: 0.9471 D(x): 0.5100\n",
            "Epoch [74/600] Batch 0/13                   Loss D: 1.0129, loss G: 1.5256 D(x): 0.6292\n",
            "Epoch [75/600] Batch 0/13                   Loss D: 1.1598, loss G: 1.6022 D(x): 0.6032\n",
            "Epoch [76/600] Batch 0/13                   Loss D: 1.2525, loss G: 1.2595 D(x): 0.4888\n",
            "Epoch [77/600] Batch 0/13                   Loss D: 1.1471, loss G: 1.3141 D(x): 0.6150\n",
            "Epoch [78/600] Batch 0/13                   Loss D: 1.4293, loss G: 2.2657 D(x): 0.8119\n",
            "Epoch [79/600] Batch 0/13                   Loss D: 1.3863, loss G: 2.1744 D(x): 0.7276\n",
            "Epoch [80/600] Batch 0/13                   Loss D: 1.1567, loss G: 1.6771 D(x): 0.7125\n",
            "Epoch [81/600] Batch 0/13                   Loss D: 1.0905, loss G: 1.3611 D(x): 0.6258\n",
            "Epoch [82/600] Batch 0/13                   Loss D: 1.0843, loss G: 2.0049 D(x): 0.7634\n",
            "Epoch [83/600] Batch 0/13                   Loss D: 1.1358, loss G: 1.8753 D(x): 0.6886\n",
            "Epoch [84/600] Batch 0/13                   Loss D: 1.1407, loss G: 1.1675 D(x): 0.5916\n",
            "Epoch [85/600] Batch 0/13                   Loss D: 1.1553, loss G: 1.2610 D(x): 0.5718\n",
            "Epoch [86/600] Batch 0/13                   Loss D: 1.1521, loss G: 1.7438 D(x): 0.6381\n",
            "Epoch [87/600] Batch 0/13                   Loss D: 1.1245, loss G: 1.7955 D(x): 0.7277\n",
            "Epoch [88/600] Batch 0/13                   Loss D: 1.1720, loss G: 1.9151 D(x): 0.6919\n",
            "Epoch [89/600] Batch 0/13                   Loss D: 1.0795, loss G: 1.7518 D(x): 0.7309\n",
            "Epoch [90/600] Batch 0/13                   Loss D: 1.1980, loss G: 2.5644 D(x): 0.8328\n",
            "Epoch [91/600] Batch 0/13                   Loss D: 1.0489, loss G: 2.0910 D(x): 0.7413\n",
            "Epoch [92/600] Batch 0/13                   Loss D: 1.0186, loss G: 1.7865 D(x): 0.7068\n",
            "Epoch [93/600] Batch 0/13                   Loss D: 1.0045, loss G: 2.0974 D(x): 0.7315\n",
            "Epoch [94/600] Batch 0/13                   Loss D: 1.0438, loss G: 1.1964 D(x): 0.5755\n",
            "Epoch [95/600] Batch 0/13                   Loss D: 0.9595, loss G: 2.0723 D(x): 0.7239\n",
            "Epoch [96/600] Batch 0/13                   Loss D: 0.9537, loss G: 2.1943 D(x): 0.7539\n",
            "Epoch [97/600] Batch 0/13                   Loss D: 0.9941, loss G: 1.4396 D(x): 0.6795\n",
            "Epoch [98/600] Batch 0/13                   Loss D: 1.0371, loss G: 1.6307 D(x): 0.6714\n",
            "Epoch [99/600] Batch 0/13                   Loss D: 1.0776, loss G: 1.7908 D(x): 0.6961\n",
            "Epoch [100/600] Batch 0/13                   Loss D: 1.1393, loss G: 2.0150 D(x): 0.7825\n",
            "Epoch [101/600] Batch 0/13                   Loss D: 1.0940, loss G: 1.3850 D(x): 0.5669\n",
            "Epoch [102/600] Batch 0/13                   Loss D: 1.0217, loss G: 2.1314 D(x): 0.7648\n",
            "Epoch [103/600] Batch 0/13                   Loss D: 0.9732, loss G: 1.7699 D(x): 0.6883\n",
            "Epoch [104/600] Batch 0/13                   Loss D: 1.0513, loss G: 1.2590 D(x): 0.5686\n",
            "Epoch [105/600] Batch 0/13                   Loss D: 1.0426, loss G: 1.4362 D(x): 0.6103\n",
            "Epoch [106/600] Batch 0/13                   Loss D: 1.0561, loss G: 1.4676 D(x): 0.6039\n",
            "Epoch [107/600] Batch 0/13                   Loss D: 1.0627, loss G: 1.7559 D(x): 0.6827\n",
            "Epoch [108/600] Batch 0/13                   Loss D: 0.9843, loss G: 2.2227 D(x): 0.7822\n",
            "Epoch [109/600] Batch 0/13                   Loss D: 1.0572, loss G: 1.1491 D(x): 0.5492\n",
            "Epoch [110/600] Batch 0/13                   Loss D: 0.9831, loss G: 1.4884 D(x): 0.6324\n",
            "Epoch [111/600] Batch 0/13                   Loss D: 1.2369, loss G: 2.3409 D(x): 0.8116\n",
            "Epoch [112/600] Batch 0/13                   Loss D: 1.0545, loss G: 1.9519 D(x): 0.7477\n",
            "Epoch [113/600] Batch 0/13                   Loss D: 1.4058, loss G: 3.6004 D(x): 0.8992\n",
            "Epoch [114/600] Batch 0/13                   Loss D: 1.1575, loss G: 1.4658 D(x): 0.5871\n",
            "Epoch [115/600] Batch 0/13                   Loss D: 0.9916, loss G: 1.6125 D(x): 0.6396\n",
            "Epoch [116/600] Batch 0/13                   Loss D: 0.9772, loss G: 1.7961 D(x): 0.7116\n",
            "Epoch [117/600] Batch 0/13                   Loss D: 0.9397, loss G: 1.5973 D(x): 0.6900\n",
            "Epoch [118/600] Batch 0/13                   Loss D: 0.9111, loss G: 1.9042 D(x): 0.7223\n",
            "Epoch [119/600] Batch 0/13                   Loss D: 0.9449, loss G: 1.9664 D(x): 0.7470\n",
            "Epoch [120/600] Batch 0/13                   Loss D: 0.9464, loss G: 1.8912 D(x): 0.7673\n",
            "Epoch [121/600] Batch 0/13                   Loss D: 0.9422, loss G: 2.0671 D(x): 0.7513\n",
            "Epoch [122/600] Batch 0/13                   Loss D: 1.0012, loss G: 2.5947 D(x): 0.8191\n",
            "Epoch [123/600] Batch 0/13                   Loss D: 0.9409, loss G: 1.5931 D(x): 0.6427\n",
            "Epoch [124/600] Batch 0/13                   Loss D: 1.0154, loss G: 2.0939 D(x): 0.7788\n",
            "Epoch [125/600] Batch 0/13                   Loss D: 0.9275, loss G: 1.9065 D(x): 0.7437\n",
            "Epoch [126/600] Batch 0/13                   Loss D: 0.9185, loss G: 1.2227 D(x): 0.6246\n",
            "Epoch [127/600] Batch 0/13                   Loss D: 0.8514, loss G: 1.8178 D(x): 0.7197\n",
            "Epoch [128/600] Batch 0/13                   Loss D: 0.9566, loss G: 1.5458 D(x): 0.6215\n",
            "Epoch [129/600] Batch 0/13                   Loss D: 0.9547, loss G: 2.0167 D(x): 0.7227\n",
            "Epoch [130/600] Batch 0/13                   Loss D: 0.9352, loss G: 2.0125 D(x): 0.7505\n",
            "Epoch [131/600] Batch 0/13                   Loss D: 0.9299, loss G: 2.3697 D(x): 0.7810\n",
            "Epoch [132/600] Batch 0/13                   Loss D: 0.8860, loss G: 1.7133 D(x): 0.7277\n",
            "Epoch [133/600] Batch 0/13                   Loss D: 1.0159, loss G: 2.4713 D(x): 0.8401\n",
            "Epoch [134/600] Batch 0/13                   Loss D: 0.9303, loss G: 2.3654 D(x): 0.7984\n",
            "Epoch [135/600] Batch 0/13                   Loss D: 0.8415, loss G: 1.7729 D(x): 0.7092\n",
            "Epoch [136/600] Batch 0/13                   Loss D: 0.8420, loss G: 1.4534 D(x): 0.6866\n",
            "Epoch [137/600] Batch 0/13                   Loss D: 0.9022, loss G: 2.4325 D(x): 0.8515\n",
            "Epoch [138/600] Batch 0/13                   Loss D: 0.8770, loss G: 2.3073 D(x): 0.8229\n",
            "Epoch [139/600] Batch 0/13                   Loss D: 0.8265, loss G: 2.0444 D(x): 0.7853\n",
            "Epoch [140/600] Batch 0/13                   Loss D: 0.9512, loss G: 2.5621 D(x): 0.8552\n",
            "Epoch [141/600] Batch 0/13                   Loss D: 0.8643, loss G: 2.2855 D(x): 0.8055\n",
            "Epoch [142/600] Batch 0/13                   Loss D: 0.8739, loss G: 2.4158 D(x): 0.8483\n",
            "Epoch [143/600] Batch 0/13                   Loss D: 0.9331, loss G: 2.6780 D(x): 0.8852\n",
            "Epoch [144/600] Batch 0/13                   Loss D: 0.8322, loss G: 1.9177 D(x): 0.7560\n",
            "Epoch [145/600] Batch 0/13                   Loss D: 0.8380, loss G: 1.7750 D(x): 0.7072\n",
            "Epoch [146/600] Batch 0/13                   Loss D: 0.8443, loss G: 1.5977 D(x): 0.6741\n",
            "Epoch [147/600] Batch 0/13                   Loss D: 0.8206, loss G: 2.1704 D(x): 0.8067\n",
            "Epoch [148/600] Batch 0/13                   Loss D: 0.8063, loss G: 2.2065 D(x): 0.8078\n",
            "Epoch [149/600] Batch 0/13                   Loss D: 0.8584, loss G: 2.4761 D(x): 0.8518\n",
            "Epoch [150/600] Batch 0/13                   Loss D: 0.7759, loss G: 2.4244 D(x): 0.8557\n",
            "Epoch [151/600] Batch 0/13                   Loss D: 0.8245, loss G: 2.0704 D(x): 0.7938\n",
            "Epoch [152/600] Batch 0/13                   Loss D: 0.7739, loss G: 2.0072 D(x): 0.7700\n",
            "Epoch [153/600] Batch 0/13                   Loss D: 0.9211, loss G: 2.9248 D(x): 0.8859\n",
            "Epoch [154/600] Batch 0/13                   Loss D: 0.8442, loss G: 2.4688 D(x): 0.8090\n",
            "Epoch [155/600] Batch 0/13                   Loss D: 0.7920, loss G: 2.1367 D(x): 0.7965\n",
            "Epoch [156/600] Batch 0/13                   Loss D: 0.7994, loss G: 2.1143 D(x): 0.7771\n",
            "Epoch [157/600] Batch 0/13                   Loss D: 0.7410, loss G: 2.1150 D(x): 0.8105\n",
            "Epoch [158/600] Batch 0/13                   Loss D: 0.8044, loss G: 2.3822 D(x): 0.8680\n",
            "Epoch [159/600] Batch 0/13                   Loss D: 0.7377, loss G: 2.2213 D(x): 0.8176\n",
            "Epoch [160/600] Batch 0/13                   Loss D: 1.3303, loss G: 3.5907 D(x): 0.9580\n",
            "Epoch [161/600] Batch 0/13                   Loss D: 0.7870, loss G: 2.2855 D(x): 0.8130\n",
            "Epoch [162/600] Batch 0/13                   Loss D: 0.7672, loss G: 2.1948 D(x): 0.7825\n",
            "Epoch [163/600] Batch 0/13                   Loss D: 0.7799, loss G: 1.9490 D(x): 0.7499\n",
            "Epoch [164/600] Batch 0/13                   Loss D: 0.7707, loss G: 1.9471 D(x): 0.7884\n",
            "Epoch [165/600] Batch 0/13                   Loss D: 0.7628, loss G: 2.5643 D(x): 0.8441\n",
            "Epoch [166/600] Batch 0/13                   Loss D: 0.7478, loss G: 2.1530 D(x): 0.8052\n",
            "Epoch [167/600] Batch 0/13                   Loss D: 0.7701, loss G: 2.5108 D(x): 0.8508\n",
            "Epoch [168/600] Batch 0/13                   Loss D: 0.7347, loss G: 2.0754 D(x): 0.8090\n",
            "Epoch [169/600] Batch 0/13                   Loss D: 0.8068, loss G: 2.9293 D(x): 0.8933\n",
            "Epoch [170/600] Batch 0/13                   Loss D: 0.7439, loss G: 2.1088 D(x): 0.7820\n",
            "Epoch [171/600] Batch 0/13                   Loss D: 0.8238, loss G: 3.2023 D(x): 0.8915\n",
            "Epoch [172/600] Batch 0/13                   Loss D: 0.7799, loss G: 2.6805 D(x): 0.8825\n",
            "Epoch [173/600] Batch 0/13                   Loss D: 0.7186, loss G: 2.4995 D(x): 0.8473\n",
            "Epoch [174/600] Batch 0/13                   Loss D: 0.7975, loss G: 1.8228 D(x): 0.7153\n",
            "Epoch [175/600] Batch 0/13                   Loss D: 0.7365, loss G: 2.2373 D(x): 0.8374\n",
            "Epoch [176/600] Batch 0/13                   Loss D: 0.7149, loss G: 2.2773 D(x): 0.8559\n",
            "Epoch [177/600] Batch 0/13                   Loss D: 0.7625, loss G: 2.8556 D(x): 0.8916\n",
            "Epoch [178/600] Batch 0/13                   Loss D: 0.7864, loss G: 3.1509 D(x): 0.9274\n",
            "Epoch [179/600] Batch 0/13                   Loss D: 0.7186, loss G: 2.2394 D(x): 0.8172\n",
            "Epoch [180/600] Batch 0/13                   Loss D: 0.7034, loss G: 2.4248 D(x): 0.8512\n",
            "Epoch [181/600] Batch 0/13                   Loss D: 0.7694, loss G: 2.9426 D(x): 0.9009\n",
            "Epoch [182/600] Batch 0/13                   Loss D: 0.7149, loss G: 2.2775 D(x): 0.8390\n",
            "Epoch [183/600] Batch 0/13                   Loss D: 0.7576, loss G: 2.9202 D(x): 0.8873\n",
            "Epoch [184/600] Batch 0/13                   Loss D: 0.7154, loss G: 2.3524 D(x): 0.8534\n",
            "Epoch [185/600] Batch 0/13                   Loss D: 0.7141, loss G: 2.0620 D(x): 0.8269\n",
            "Epoch [186/600] Batch 0/13                   Loss D: 0.7080, loss G: 2.0971 D(x): 0.8193\n",
            "Epoch [187/600] Batch 0/13                   Loss D: 0.9233, loss G: 3.5186 D(x): 0.9210\n",
            "Epoch [188/600] Batch 0/13                   Loss D: 0.7582, loss G: 1.9373 D(x): 0.7634\n",
            "Epoch [189/600] Batch 0/13                   Loss D: 0.7447, loss G: 2.6372 D(x): 0.8870\n",
            "Epoch [190/600] Batch 0/13                   Loss D: 0.7130, loss G: 2.5422 D(x): 0.8823\n",
            "Epoch [191/600] Batch 0/13                   Loss D: 0.7013, loss G: 2.3681 D(x): 0.8512\n",
            "Epoch [192/600] Batch 0/13                   Loss D: 0.7070, loss G: 2.1947 D(x): 0.8180\n",
            "Epoch [193/600] Batch 0/13                   Loss D: 0.7725, loss G: 3.0234 D(x): 0.9164\n",
            "Epoch [194/600] Batch 0/13                   Loss D: 0.7110, loss G: 2.3757 D(x): 0.8617\n",
            "Epoch [195/600] Batch 0/13                   Loss D: 0.7096, loss G: 2.6438 D(x): 0.9023\n",
            "Epoch [196/600] Batch 0/13                   Loss D: 0.7047, loss G: 2.3949 D(x): 0.8380\n",
            "Epoch [197/600] Batch 0/13                   Loss D: 0.7045, loss G: 2.6121 D(x): 0.8721\n",
            "Epoch [198/600] Batch 0/13                   Loss D: 0.7125, loss G: 2.4073 D(x): 0.8422\n",
            "Epoch [199/600] Batch 0/13                   Loss D: 0.7040, loss G: 2.2171 D(x): 0.8239\n",
            "Epoch [200/600] Batch 0/13                   Loss D: 0.7528, loss G: 2.8428 D(x): 0.9113\n",
            "Epoch [201/600] Batch 0/13                   Loss D: 0.7353, loss G: 2.9544 D(x): 0.9029\n",
            "Epoch [202/600] Batch 0/13                   Loss D: 0.7188, loss G: 2.7849 D(x): 0.8875\n",
            "Epoch [203/600] Batch 0/13                   Loss D: 0.7206, loss G: 2.1542 D(x): 0.7977\n",
            "Epoch [204/600] Batch 0/13                   Loss D: 0.7117, loss G: 1.8650 D(x): 0.8046\n",
            "Epoch [205/600] Batch 0/13                   Loss D: 0.7062, loss G: 2.0518 D(x): 0.8419\n",
            "Epoch [206/600] Batch 0/13                   Loss D: 0.7185, loss G: 2.7529 D(x): 0.8777\n",
            "Epoch [207/600] Batch 0/13                   Loss D: 0.8230, loss G: 1.7929 D(x): 0.6998\n",
            "Epoch [208/600] Batch 0/13                   Loss D: 1.2080, loss G: 1.8536 D(x): 0.5593\n",
            "Epoch [209/600] Batch 0/13                   Loss D: 0.8628, loss G: 2.4610 D(x): 0.7665\n",
            "Epoch [210/600] Batch 0/13                   Loss D: 0.8259, loss G: 2.9713 D(x): 0.8422\n",
            "Epoch [211/600] Batch 0/13                   Loss D: 0.7788, loss G: 2.4860 D(x): 0.7805\n",
            "Epoch [212/600] Batch 0/13                   Loss D: 0.7389, loss G: 2.4167 D(x): 0.8133\n",
            "Epoch [213/600] Batch 0/13                   Loss D: 0.7137, loss G: 2.2669 D(x): 0.8341\n",
            "Epoch [214/600] Batch 0/13                   Loss D: 0.7171, loss G: 2.3340 D(x): 0.8518\n",
            "Epoch [215/600] Batch 0/13                   Loss D: 0.6870, loss G: 2.1602 D(x): 0.8594\n",
            "Epoch [216/600] Batch 0/13                   Loss D: 0.7093, loss G: 2.1755 D(x): 0.8565\n",
            "Epoch [217/600] Batch 0/13                   Loss D: 0.7033, loss G: 2.4610 D(x): 0.8655\n",
            "Epoch [218/600] Batch 0/13                   Loss D: 0.7169, loss G: 2.7096 D(x): 0.9096\n",
            "Epoch [219/600] Batch 0/13                   Loss D: 0.6858, loss G: 2.1709 D(x): 0.8467\n",
            "Epoch [220/600] Batch 0/13                   Loss D: 0.6855, loss G: 2.4334 D(x): 0.8650\n",
            "Epoch [221/600] Batch 0/13                   Loss D: 0.7030, loss G: 2.5528 D(x): 0.8811\n",
            "Epoch [222/600] Batch 0/13                   Loss D: 0.6859, loss G: 2.2431 D(x): 0.8586\n",
            "Epoch [223/600] Batch 0/13                   Loss D: 0.6969, loss G: 2.5029 D(x): 0.8784\n",
            "Epoch [224/600] Batch 0/13                   Loss D: 0.6897, loss G: 2.3273 D(x): 0.8750\n",
            "Epoch [225/600] Batch 0/13                   Loss D: 0.7132, loss G: 2.6660 D(x): 0.9141\n",
            "Epoch [226/600] Batch 0/13                   Loss D: 0.6922, loss G: 2.3229 D(x): 0.8452\n",
            "Epoch [227/600] Batch 0/13                   Loss D: 0.6769, loss G: 2.2604 D(x): 0.8662\n",
            "Epoch [228/600] Batch 0/13                   Loss D: 0.6917, loss G: 2.3753 D(x): 0.8827\n",
            "Epoch [229/600] Batch 0/13                   Loss D: 0.6924, loss G: 2.5872 D(x): 0.8898\n",
            "Epoch [230/600] Batch 0/13                   Loss D: 0.6912, loss G: 2.6446 D(x): 0.8862\n",
            "Epoch [231/600] Batch 0/13                   Loss D: 0.6791, loss G: 2.2818 D(x): 0.8601\n",
            "Epoch [232/600] Batch 0/13                   Loss D: 0.7349, loss G: 3.3656 D(x): 0.9264\n",
            "Epoch [233/600] Batch 0/13                   Loss D: 1.5008, loss G: 3.2283 D(x): 0.8509\n",
            "Epoch [234/600] Batch 0/13                   Loss D: 0.8657, loss G: 2.7681 D(x): 0.7860\n",
            "Epoch [235/600] Batch 0/13                   Loss D: 0.7999, loss G: 2.7001 D(x): 0.8796\n",
            "Epoch [236/600] Batch 0/13                   Loss D: 0.7558, loss G: 2.6840 D(x): 0.8747\n",
            "Epoch [237/600] Batch 0/13                   Loss D: 0.7535, loss G: 2.4336 D(x): 0.8540\n",
            "Epoch [238/600] Batch 0/13                   Loss D: 0.7208, loss G: 2.5002 D(x): 0.8855\n",
            "Epoch [239/600] Batch 0/13                   Loss D: 0.6906, loss G: 2.3554 D(x): 0.8639\n",
            "Epoch [240/600] Batch 0/13                   Loss D: 0.7017, loss G: 2.7101 D(x): 0.9111\n",
            "Epoch [241/600] Batch 0/13                   Loss D: 0.7109, loss G: 2.5001 D(x): 0.8896\n",
            "Epoch [242/600] Batch 0/13                   Loss D: 0.7285, loss G: 2.7462 D(x): 0.9230\n",
            "Epoch [243/600] Batch 0/13                   Loss D: 0.7409, loss G: 2.7610 D(x): 0.9240\n",
            "Epoch [244/600] Batch 0/13                   Loss D: 0.6800, loss G: 2.4387 D(x): 0.8860\n",
            "Epoch [245/600] Batch 0/13                   Loss D: 0.6805, loss G: 2.5278 D(x): 0.9013\n",
            "Epoch [246/600] Batch 0/13                   Loss D: 0.6797, loss G: 2.4183 D(x): 0.8745\n",
            "Epoch [247/600] Batch 0/13                   Loss D: 0.6771, loss G: 2.3775 D(x): 0.8692\n",
            "Epoch [248/600] Batch 0/13                   Loss D: 0.6808, loss G: 2.4320 D(x): 0.8689\n",
            "Epoch [249/600] Batch 0/13                   Loss D: 0.6933, loss G: 2.9368 D(x): 0.9067\n",
            "Epoch [250/600] Batch 0/13                   Loss D: 0.6858, loss G: 2.2630 D(x): 0.8494\n",
            "Epoch [251/600] Batch 0/13                   Loss D: 0.7084, loss G: 1.9414 D(x): 0.8082\n",
            "Epoch [252/600] Batch 0/13                   Loss D: 0.7018, loss G: 2.5511 D(x): 0.8865\n",
            "Epoch [253/600] Batch 0/13                   Loss D: 0.7023, loss G: 2.9821 D(x): 0.9140\n",
            "Epoch [254/600] Batch 0/13                   Loss D: 0.6918, loss G: 2.6978 D(x): 0.8940\n",
            "Epoch [255/600] Batch 0/13                   Loss D: 0.6829, loss G: 2.3151 D(x): 0.8587\n",
            "Epoch [256/600] Batch 0/13                   Loss D: 0.6780, loss G: 2.4014 D(x): 0.8730\n",
            "Epoch [257/600] Batch 0/13                   Loss D: 0.6938, loss G: 2.1339 D(x): 0.8636\n",
            "Epoch [258/600] Batch 0/13                   Loss D: 0.6988, loss G: 1.8832 D(x): 0.8200\n",
            "Epoch [259/600] Batch 0/13                   Loss D: 0.7013, loss G: 2.9486 D(x): 0.9186\n",
            "Epoch [260/600] Batch 0/13                   Loss D: 0.7087, loss G: 2.9454 D(x): 0.9055\n",
            "Epoch [261/600] Batch 0/13                   Loss D: 0.7002, loss G: 3.0399 D(x): 0.9086\n",
            "Epoch [262/600] Batch 0/13                   Loss D: 0.6934, loss G: 2.8227 D(x): 0.8943\n",
            "Epoch [263/600] Batch 0/13                   Loss D: 0.6841, loss G: 2.3566 D(x): 0.8683\n",
            "Epoch [264/600] Batch 0/13                   Loss D: 0.6964, loss G: 2.7124 D(x): 0.9192\n",
            "Epoch [265/600] Batch 0/13                   Loss D: 0.7052, loss G: 2.8297 D(x): 0.9186\n",
            "Epoch [266/600] Batch 0/13                   Loss D: 0.6826, loss G: 2.5436 D(x): 0.8887\n",
            "Epoch [267/600] Batch 0/13                   Loss D: 0.7032, loss G: 1.8343 D(x): 0.8100\n",
            "Epoch [268/600] Batch 0/13                   Loss D: 0.7052, loss G: 2.7976 D(x): 0.9250\n",
            "Epoch [269/600] Batch 0/13                   Loss D: 0.6787, loss G: 2.5468 D(x): 0.9020\n",
            "Epoch [270/600] Batch 0/13                   Loss D: 0.7696, loss G: 3.8756 D(x): 0.9506\n",
            "Epoch [271/600] Batch 0/13                   Loss D: 0.7542, loss G: 3.0327 D(x): 0.8716\n",
            "Epoch [272/600] Batch 0/13                   Loss D: 0.6935, loss G: 2.7757 D(x): 0.8695\n",
            "Epoch [273/600] Batch 0/13                   Loss D: 0.6869, loss G: 2.5547 D(x): 0.8821\n",
            "Epoch [274/600] Batch 0/13                   Loss D: 0.6847, loss G: 2.6213 D(x): 0.8911\n",
            "Epoch [275/600] Batch 0/13                   Loss D: 0.7310, loss G: 3.2749 D(x): 0.9170\n",
            "Epoch [276/600] Batch 0/13                   Loss D: 0.6775, loss G: 2.3130 D(x): 0.8582\n",
            "Epoch [277/600] Batch 0/13                   Loss D: 0.6839, loss G: 2.5759 D(x): 0.9079\n",
            "Epoch [278/600] Batch 0/13                   Loss D: 0.6861, loss G: 2.4986 D(x): 0.9002\n",
            "Epoch [279/600] Batch 0/13                   Loss D: 0.6824, loss G: 2.3517 D(x): 0.8512\n",
            "Epoch [280/600] Batch 0/13                   Loss D: 0.6765, loss G: 2.3487 D(x): 0.8823\n",
            "Epoch [281/600] Batch 0/13                   Loss D: 0.6994, loss G: 1.9893 D(x): 0.8320\n",
            "Epoch [282/600] Batch 0/13                   Loss D: 0.6722, loss G: 2.5244 D(x): 0.9103\n",
            "Epoch [283/600] Batch 0/13                   Loss D: 0.6783, loss G: 2.3644 D(x): 0.8893\n",
            "Epoch [284/600] Batch 0/13                   Loss D: 0.6984, loss G: 2.9089 D(x): 0.9086\n",
            "Epoch [285/600] Batch 0/13                   Loss D: 0.6865, loss G: 2.2062 D(x): 0.8445\n",
            "Epoch [286/600] Batch 0/13                   Loss D: 0.7074, loss G: 2.7962 D(x): 0.9212\n",
            "Epoch [287/600] Batch 0/13                   Loss D: 0.7142, loss G: 1.8095 D(x): 0.8065\n",
            "Epoch [288/600] Batch 0/13                   Loss D: 0.6813, loss G: 2.4770 D(x): 0.8867\n",
            "Epoch [289/600] Batch 0/13                   Loss D: 0.6701, loss G: 2.3483 D(x): 0.9066\n",
            "Epoch [290/600] Batch 0/13                   Loss D: 0.7952, loss G: 3.4384 D(x): 0.9523\n",
            "Epoch [291/600] Batch 0/13                   Loss D: 0.6794, loss G: 2.5528 D(x): 0.8710\n",
            "Epoch [292/600] Batch 0/13                   Loss D: 0.6769, loss G: 2.3755 D(x): 0.8896\n",
            "Epoch [293/600] Batch 0/13                   Loss D: 0.7317, loss G: 3.0300 D(x): 0.9320\n",
            "Epoch [294/600] Batch 0/13                   Loss D: 0.6721, loss G: 2.6395 D(x): 0.9034\n",
            "Epoch [295/600] Batch 0/13                   Loss D: 0.6844, loss G: 2.7090 D(x): 0.9082\n",
            "Epoch [296/600] Batch 0/13                   Loss D: 0.6892, loss G: 2.1906 D(x): 0.8625\n",
            "Epoch [297/600] Batch 0/13                   Loss D: 0.6779, loss G: 2.7441 D(x): 0.8991\n",
            "Epoch [298/600] Batch 0/13                   Loss D: 0.6788, loss G: 2.7494 D(x): 0.9176\n",
            "Epoch [299/600] Batch 0/13                   Loss D: 0.8021, loss G: 4.7425 D(x): 0.9372\n",
            "Epoch [300/600] Batch 0/13                   Loss D: 1.3610, loss G: 2.6961 D(x): 0.7723\n",
            "Epoch [301/600] Batch 0/13                   Loss D: 0.9159, loss G: 2.9206 D(x): 0.7870\n",
            "Epoch [302/600] Batch 0/13                   Loss D: 0.9923, loss G: 2.8690 D(x): 0.7949\n",
            "Epoch [303/600] Batch 0/13                   Loss D: 0.8116, loss G: 2.0914 D(x): 0.7340\n",
            "Epoch [304/600] Batch 0/13                   Loss D: 0.8010, loss G: 2.5440 D(x): 0.8547\n",
            "Epoch [305/600] Batch 0/13                   Loss D: 0.7232, loss G: 2.4433 D(x): 0.8122\n",
            "Epoch [306/600] Batch 0/13                   Loss D: 0.7277, loss G: 2.4786 D(x): 0.8521\n",
            "Epoch [307/600] Batch 0/13                   Loss D: 0.7373, loss G: 2.5279 D(x): 0.8526\n",
            "Epoch [308/600] Batch 0/13                   Loss D: 0.7061, loss G: 2.6801 D(x): 0.8869\n",
            "Epoch [309/600] Batch 0/13                   Loss D: 0.7227, loss G: 2.9100 D(x): 0.9086\n",
            "Epoch [310/600] Batch 0/13                   Loss D: 0.7002, loss G: 2.4521 D(x): 0.8462\n",
            "Epoch [311/600] Batch 0/13                   Loss D: 0.7095, loss G: 2.5443 D(x): 0.8852\n",
            "Epoch [312/600] Batch 0/13                   Loss D: 0.7049, loss G: 2.3474 D(x): 0.8346\n",
            "Epoch [313/600] Batch 0/13                   Loss D: 0.6863, loss G: 2.3271 D(x): 0.8465\n",
            "Epoch [314/600] Batch 0/13                   Loss D: 0.6788, loss G: 2.3076 D(x): 0.8678\n",
            "Epoch [315/600] Batch 0/13                   Loss D: 0.7055, loss G: 2.7928 D(x): 0.9175\n",
            "Epoch [316/600] Batch 0/13                   Loss D: 0.6838, loss G: 2.6322 D(x): 0.9000\n",
            "Epoch [317/600] Batch 0/13                   Loss D: 0.6815, loss G: 2.3764 D(x): 0.8750\n",
            "Epoch [318/600] Batch 0/13                   Loss D: 0.6714, loss G: 2.4044 D(x): 0.8910\n",
            "Epoch [319/600] Batch 0/13                   Loss D: 0.6782, loss G: 2.5077 D(x): 0.8891\n",
            "Epoch [320/600] Batch 0/13                   Loss D: 0.6752, loss G: 2.4730 D(x): 0.8824\n",
            "Epoch [321/600] Batch 0/13                   Loss D: 0.6829, loss G: 2.1280 D(x): 0.8493\n",
            "Epoch [322/600] Batch 0/13                   Loss D: 0.6724, loss G: 2.6114 D(x): 0.9047\n",
            "Epoch [323/600] Batch 0/13                   Loss D: 0.6939, loss G: 2.7509 D(x): 0.8771\n",
            "Epoch [324/600] Batch 0/13                   Loss D: 0.6842, loss G: 2.7041 D(x): 0.9207\n",
            "Epoch [325/600] Batch 0/13                   Loss D: 0.6806, loss G: 2.3321 D(x): 0.8706\n",
            "Epoch [326/600] Batch 0/13                   Loss D: 0.6748, loss G: 2.3730 D(x): 0.8716\n",
            "Epoch [327/600] Batch 0/13                   Loss D: 0.6799, loss G: 2.3753 D(x): 0.8916\n",
            "Epoch [328/600] Batch 0/13                   Loss D: 0.6825, loss G: 2.7111 D(x): 0.9100\n",
            "Epoch [329/600] Batch 0/13                   Loss D: 0.6775, loss G: 2.2955 D(x): 0.8645\n",
            "Epoch [330/600] Batch 0/13                   Loss D: 0.6877, loss G: 2.0686 D(x): 0.8389\n",
            "Epoch [331/600] Batch 0/13                   Loss D: 0.6946, loss G: 1.7092 D(x): 0.8245\n",
            "Epoch [332/600] Batch 0/13                   Loss D: 0.6754, loss G: 2.2522 D(x): 0.8835\n",
            "Epoch [333/600] Batch 0/13                   Loss D: 0.6802, loss G: 2.5967 D(x): 0.8876\n",
            "Epoch [334/600] Batch 0/13                   Loss D: 0.6942, loss G: 3.0569 D(x): 0.9252\n",
            "Epoch [335/600] Batch 0/13                   Loss D: 0.6895, loss G: 2.7463 D(x): 0.9030\n",
            "Epoch [336/600] Batch 0/13                   Loss D: 0.6801, loss G: 2.7853 D(x): 0.9058\n",
            "Epoch [337/600] Batch 0/13                   Loss D: 0.6912, loss G: 2.5896 D(x): 0.9047\n",
            "Epoch [338/600] Batch 0/13                   Loss D: 0.6829, loss G: 2.2241 D(x): 0.8541\n",
            "Epoch [339/600] Batch 0/13                   Loss D: 0.7002, loss G: 3.0395 D(x): 0.9149\n",
            "Epoch [340/600] Batch 0/13                   Loss D: 0.6897, loss G: 2.9002 D(x): 0.9002\n",
            "Epoch [341/600] Batch 0/13                   Loss D: 0.6736, loss G: 2.6117 D(x): 0.9012\n",
            "Epoch [342/600] Batch 0/13                   Loss D: 0.6939, loss G: 2.5827 D(x): 0.9220\n",
            "Epoch [343/600] Batch 0/13                   Loss D: 0.6768, loss G: 2.6921 D(x): 0.8985\n",
            "Epoch [344/600] Batch 0/13                   Loss D: 0.6852, loss G: 2.3342 D(x): 0.8585\n",
            "Epoch [345/600] Batch 0/13                   Loss D: 0.6847, loss G: 1.8681 D(x): 0.8366\n",
            "Epoch [346/600] Batch 0/13                   Loss D: 0.6856, loss G: 2.8351 D(x): 0.9167\n",
            "Epoch [347/600] Batch 0/13                   Loss D: 0.7028, loss G: 3.0800 D(x): 0.9309\n",
            "Epoch [348/600] Batch 0/13                   Loss D: 0.7337, loss G: 3.3087 D(x): 0.9349\n",
            "Epoch [349/600] Batch 0/13                   Loss D: 0.6827, loss G: 2.4001 D(x): 0.8752\n",
            "Epoch [350/600] Batch 0/13                   Loss D: 0.6851, loss G: 2.4347 D(x): 0.8678\n",
            "Epoch [351/600] Batch 0/13                   Loss D: 0.6951, loss G: 2.9385 D(x): 0.9263\n",
            "Epoch [352/600] Batch 0/13                   Loss D: 0.6962, loss G: 1.9141 D(x): 0.8336\n",
            "Epoch [353/600] Batch 0/13                   Loss D: 0.6844, loss G: 2.5529 D(x): 0.8985\n",
            "Epoch [354/600] Batch 0/13                   Loss D: 0.6722, loss G: 2.4872 D(x): 0.9181\n",
            "Epoch [355/600] Batch 0/13                   Loss D: 0.7003, loss G: 3.1148 D(x): 0.9236\n",
            "Epoch [356/600] Batch 0/13                   Loss D: 0.6830, loss G: 2.5717 D(x): 0.8874\n",
            "Epoch [357/600] Batch 0/13                   Loss D: 0.6774, loss G: 2.3505 D(x): 0.8900\n",
            "Epoch [358/600] Batch 0/13                   Loss D: 0.6808, loss G: 2.2260 D(x): 0.8814\n",
            "Epoch [359/600] Batch 0/13                   Loss D: 0.6754, loss G: 2.2889 D(x): 0.8666\n",
            "Epoch [360/600] Batch 0/13                   Loss D: 0.6972, loss G: 2.9241 D(x): 0.9148\n",
            "Epoch [361/600] Batch 0/13                   Loss D: 0.6773, loss G: 2.4786 D(x): 0.9101\n",
            "Epoch [362/600] Batch 0/13                   Loss D: 0.6710, loss G: 2.6424 D(x): 0.9091\n",
            "Epoch [363/600] Batch 0/13                   Loss D: 0.6710, loss G: 2.3189 D(x): 0.8797\n",
            "Epoch [364/600] Batch 0/13                   Loss D: 0.7119, loss G: 3.1516 D(x): 0.9300\n",
            "Epoch [365/600] Batch 0/13                   Loss D: 0.6918, loss G: 2.6470 D(x): 0.8615\n",
            "Epoch [366/600] Batch 0/13                   Loss D: 0.6761, loss G: 2.3786 D(x): 0.8825\n",
            "Epoch [367/600] Batch 0/13                   Loss D: 0.6763, loss G: 2.7478 D(x): 0.9152\n",
            "Epoch [368/600] Batch 0/13                   Loss D: 0.6864, loss G: 2.8348 D(x): 0.9174\n",
            "Epoch [369/600] Batch 0/13                   Loss D: 0.7283, loss G: 3.2874 D(x): 0.9385\n",
            "Epoch [370/600] Batch 0/13                   Loss D: 0.6742, loss G: 2.4363 D(x): 0.8949\n",
            "Epoch [371/600] Batch 0/13                   Loss D: 0.6950, loss G: 2.7757 D(x): 0.9288\n",
            "Epoch [372/600] Batch 0/13                   Loss D: 0.7087, loss G: 2.9034 D(x): 0.9376\n",
            "Epoch [373/600] Batch 0/13                   Loss D: 0.6753, loss G: 2.8255 D(x): 0.9050\n",
            "Epoch [374/600] Batch 0/13                   Loss D: 0.6904, loss G: 2.9998 D(x): 0.9254\n",
            "Epoch [375/600] Batch 0/13                   Loss D: 0.6913, loss G: 1.7950 D(x): 0.8333\n",
            "Epoch [376/600] Batch 0/13                   Loss D: 0.6728, loss G: 2.4547 D(x): 0.8945\n",
            "Epoch [377/600] Batch 0/13                   Loss D: 0.6845, loss G: 3.1694 D(x): 0.9218\n",
            "Epoch [378/600] Batch 0/13                   Loss D: 0.6772, loss G: 2.6913 D(x): 0.8982\n",
            "Epoch [379/600] Batch 0/13                   Loss D: 0.7063, loss G: 2.9892 D(x): 0.9339\n",
            "Epoch [380/600] Batch 0/13                   Loss D: 0.6730, loss G: 2.5795 D(x): 0.9021\n",
            "Epoch [381/600] Batch 0/13                   Loss D: 0.6897, loss G: 1.9044 D(x): 0.8386\n",
            "Epoch [382/600] Batch 0/13                   Loss D: 0.7131, loss G: 3.4434 D(x): 0.9303\n",
            "Epoch [383/600] Batch 0/13                   Loss D: 1.6428, loss G: 0.7476 D(x): 0.3698\n",
            "Epoch [384/600] Batch 0/13                   Loss D: 0.9727, loss G: 2.9822 D(x): 0.7268\n",
            "Epoch [385/600] Batch 0/13                   Loss D: 0.9134, loss G: 2.6400 D(x): 0.7327\n",
            "Epoch [386/600] Batch 0/13                   Loss D: 0.8058, loss G: 2.8741 D(x): 0.8145\n",
            "Epoch [387/600] Batch 0/13                   Loss D: 0.8088, loss G: 2.8432 D(x): 0.8581\n",
            "Epoch [388/600] Batch 0/13                   Loss D: 0.7194, loss G: 2.6122 D(x): 0.8753\n",
            "Epoch [389/600] Batch 0/13                   Loss D: 0.6870, loss G: 2.2671 D(x): 0.8675\n",
            "Epoch [390/600] Batch 0/13                   Loss D: 0.7189, loss G: 2.6295 D(x): 0.8735\n",
            "Epoch [391/600] Batch 0/13                   Loss D: 0.6836, loss G: 2.5489 D(x): 0.8672\n",
            "Epoch [392/600] Batch 0/13                   Loss D: 0.6839, loss G: 2.4457 D(x): 0.8931\n",
            "Epoch [393/600] Batch 0/13                   Loss D: 0.6747, loss G: 2.3912 D(x): 0.8810\n",
            "Epoch [394/600] Batch 0/13                   Loss D: 0.6849, loss G: 2.6959 D(x): 0.8874\n",
            "Epoch [395/600] Batch 0/13                   Loss D: 0.6713, loss G: 2.4842 D(x): 0.8863\n",
            "Epoch [396/600] Batch 0/13                   Loss D: 0.6776, loss G: 2.2042 D(x): 0.8582\n",
            "Epoch [397/600] Batch 0/13                   Loss D: 0.6738, loss G: 2.4908 D(x): 0.8827\n",
            "Epoch [398/600] Batch 0/13                   Loss D: 0.6750, loss G: 2.3925 D(x): 0.8805\n",
            "Epoch [399/600] Batch 0/13                   Loss D: 0.6842, loss G: 2.6546 D(x): 0.9148\n",
            "Epoch [400/600] Batch 0/13                   Loss D: 0.6943, loss G: 2.7209 D(x): 0.9064\n",
            "Epoch [401/600] Batch 0/13                   Loss D: 0.6764, loss G: 2.2470 D(x): 0.8843\n",
            "Epoch [402/600] Batch 0/13                   Loss D: 0.6798, loss G: 2.5498 D(x): 0.9088\n",
            "Epoch [403/600] Batch 0/13                   Loss D: 0.6677, loss G: 2.4910 D(x): 0.8995\n",
            "Epoch [404/600] Batch 0/13                   Loss D: 0.7005, loss G: 2.9036 D(x): 0.9059\n",
            "Epoch [405/600] Batch 0/13                   Loss D: 0.6740, loss G: 2.3478 D(x): 0.8693\n",
            "Epoch [406/600] Batch 0/13                   Loss D: 0.6751, loss G: 2.4579 D(x): 0.8918\n",
            "Epoch [407/600] Batch 0/13                   Loss D: 0.6822, loss G: 2.2459 D(x): 0.8547\n",
            "Epoch [408/600] Batch 0/13                   Loss D: 0.6745, loss G: 2.6994 D(x): 0.9149\n",
            "Epoch [409/600] Batch 0/13                   Loss D: 0.6888, loss G: 2.7940 D(x): 0.9130\n",
            "Epoch [410/600] Batch 0/13                   Loss D: 0.6833, loss G: 2.3399 D(x): 0.8573\n",
            "Epoch [411/600] Batch 0/13                   Loss D: 0.6710, loss G: 2.3968 D(x): 0.8934\n",
            "Epoch [412/600] Batch 0/13                   Loss D: 0.6722, loss G: 2.3188 D(x): 0.8871\n",
            "Epoch [413/600] Batch 0/13                   Loss D: 0.6738, loss G: 2.4886 D(x): 0.8861\n",
            "Epoch [414/600] Batch 0/13                   Loss D: 0.6953, loss G: 3.0556 D(x): 0.9257\n",
            "Epoch [415/600] Batch 0/13                   Loss D: 0.6780, loss G: 2.6670 D(x): 0.9121\n",
            "Epoch [416/600] Batch 0/13                   Loss D: 0.6688, loss G: 2.4386 D(x): 0.8975\n",
            "Epoch [417/600] Batch 0/13                   Loss D: 0.6776, loss G: 2.4901 D(x): 0.8992\n",
            "Epoch [418/600] Batch 0/13                   Loss D: 0.6751, loss G: 2.5483 D(x): 0.8989\n",
            "Epoch [419/600] Batch 0/13                   Loss D: 0.6708, loss G: 2.6006 D(x): 0.8891\n",
            "Epoch [420/600] Batch 0/13                   Loss D: 0.6738, loss G: 2.1024 D(x): 0.8591\n",
            "Epoch [421/600] Batch 0/13                   Loss D: 0.6735, loss G: 2.6749 D(x): 0.8952\n",
            "Epoch [422/600] Batch 0/13                   Loss D: 0.6714, loss G: 2.6152 D(x): 0.8931\n",
            "Epoch [423/600] Batch 0/13                   Loss D: 0.6845, loss G: 2.7807 D(x): 0.9026\n",
            "Epoch [424/600] Batch 0/13                   Loss D: 0.6769, loss G: 2.5858 D(x): 0.9048\n",
            "Epoch [425/600] Batch 0/13                   Loss D: 0.6746, loss G: 2.7275 D(x): 0.8985\n",
            "Epoch [426/600] Batch 0/13                   Loss D: 0.6770, loss G: 2.0011 D(x): 0.8618\n",
            "Epoch [427/600] Batch 0/13                   Loss D: 0.6751, loss G: 2.2008 D(x): 0.8676\n",
            "Epoch [428/600] Batch 0/13                   Loss D: 0.6728, loss G: 2.4230 D(x): 0.8914\n",
            "Epoch [429/600] Batch 0/13                   Loss D: 0.6725, loss G: 2.3898 D(x): 0.8781\n",
            "Epoch [430/600] Batch 0/13                   Loss D: 0.6780, loss G: 2.5989 D(x): 0.9121\n",
            "Epoch [431/600] Batch 0/13                   Loss D: 0.6827, loss G: 2.1779 D(x): 0.8558\n",
            "Epoch [432/600] Batch 0/13                   Loss D: 0.6837, loss G: 2.7377 D(x): 0.9199\n",
            "Epoch [433/600] Batch 0/13                   Loss D: 0.6737, loss G: 2.3449 D(x): 0.8768\n",
            "Epoch [434/600] Batch 0/13                   Loss D: 0.6697, loss G: 2.2184 D(x): 0.8784\n",
            "Epoch [435/600] Batch 0/13                   Loss D: 0.6769, loss G: 2.7366 D(x): 0.9179\n",
            "Epoch [436/600] Batch 0/13                   Loss D: 0.6768, loss G: 2.4910 D(x): 0.9029\n",
            "Epoch [437/600] Batch 0/13                   Loss D: 0.6746, loss G: 2.5712 D(x): 0.9049\n",
            "Epoch [438/600] Batch 0/13                   Loss D: 0.6763, loss G: 2.6196 D(x): 0.8928\n",
            "Epoch [439/600] Batch 0/13                   Loss D: 0.6806, loss G: 2.8539 D(x): 0.9188\n",
            "Epoch [440/600] Batch 0/13                   Loss D: 0.6767, loss G: 2.5390 D(x): 0.9036\n",
            "Epoch [441/600] Batch 0/13                   Loss D: 0.6728, loss G: 2.2549 D(x): 0.8710\n",
            "Epoch [442/600] Batch 0/13                   Loss D: 0.6728, loss G: 2.6425 D(x): 0.9068\n",
            "Epoch [443/600] Batch 0/13                   Loss D: 0.6789, loss G: 2.0761 D(x): 0.8583\n",
            "Epoch [444/600] Batch 0/13                   Loss D: 0.6758, loss G: 2.4207 D(x): 0.8809\n",
            "Epoch [445/600] Batch 0/13                   Loss D: 0.6737, loss G: 2.5714 D(x): 0.9117\n",
            "Epoch [446/600] Batch 0/13                   Loss D: 0.6709, loss G: 2.4609 D(x): 0.9026\n",
            "Epoch [447/600] Batch 0/13                   Loss D: 0.6673, loss G: 2.3931 D(x): 0.8875\n",
            "Epoch [448/600] Batch 0/13                   Loss D: 0.7070, loss G: 3.0889 D(x): 0.9309\n",
            "Epoch [449/600] Batch 0/13                   Loss D: 0.6726, loss G: 2.2024 D(x): 0.8856\n",
            "Epoch [450/600] Batch 0/13                   Loss D: 0.6743, loss G: 2.4571 D(x): 0.8905\n",
            "Epoch [451/600] Batch 0/13                   Loss D: 0.6761, loss G: 2.6244 D(x): 0.9176\n",
            "Epoch [452/600] Batch 0/13                   Loss D: 0.6812, loss G: 2.7070 D(x): 0.9049\n",
            "Epoch [453/600] Batch 0/13                   Loss D: 0.6749, loss G: 2.1927 D(x): 0.8758\n",
            "Epoch [454/600] Batch 0/13                   Loss D: 0.6863, loss G: 2.2773 D(x): 0.8617\n",
            "Epoch [455/600] Batch 0/13                   Loss D: 0.6643, loss G: 2.2879 D(x): 0.8854\n",
            "Epoch [456/600] Batch 0/13                   Loss D: 0.6705, loss G: 2.4293 D(x): 0.9140\n",
            "Epoch [457/600] Batch 0/13                   Loss D: 0.6649, loss G: 2.4436 D(x): 0.8939\n",
            "Epoch [458/600] Batch 0/13                   Loss D: 0.6663, loss G: 2.6511 D(x): 0.8869\n",
            "Epoch [459/600] Batch 0/13                   Loss D: 0.6801, loss G: 2.6006 D(x): 0.9048\n",
            "Epoch [460/600] Batch 0/13                   Loss D: 0.6793, loss G: 2.8376 D(x): 0.9146\n",
            "Epoch [461/600] Batch 0/13                   Loss D: 0.6931, loss G: 3.2560 D(x): 0.9306\n",
            "Epoch [462/600] Batch 0/13                   Loss D: 0.6724, loss G: 2.2306 D(x): 0.8850\n",
            "Epoch [463/600] Batch 0/13                   Loss D: 0.6751, loss G: 2.5573 D(x): 0.9119\n",
            "Epoch [464/600] Batch 0/13                   Loss D: 0.6727, loss G: 2.4585 D(x): 0.8930\n",
            "Epoch [465/600] Batch 0/13                   Loss D: 0.6702, loss G: 2.3211 D(x): 0.8777\n",
            "Epoch [466/600] Batch 0/13                   Loss D: 0.6780, loss G: 2.0406 D(x): 0.8600\n",
            "Epoch [467/600] Batch 0/13                   Loss D: 0.6757, loss G: 2.6864 D(x): 0.9102\n",
            "Epoch [468/600] Batch 0/13                   Loss D: 0.6787, loss G: 2.3151 D(x): 0.8792\n",
            "Epoch [469/600] Batch 0/13                   Loss D: 0.6748, loss G: 2.4932 D(x): 0.8979\n",
            "Epoch [470/600] Batch 0/13                   Loss D: 0.6729, loss G: 2.7888 D(x): 0.9239\n",
            "Epoch [471/600] Batch 0/13                   Loss D: 0.6721, loss G: 2.2559 D(x): 0.8804\n",
            "Epoch [472/600] Batch 0/13                   Loss D: 0.6840, loss G: 2.6889 D(x): 0.9252\n",
            "Epoch [473/600] Batch 0/13                   Loss D: 0.6768, loss G: 2.5425 D(x): 0.9056\n",
            "Epoch [474/600] Batch 0/13                   Loss D: 0.6732, loss G: 2.1813 D(x): 0.8743\n",
            "Epoch [475/600] Batch 0/13                   Loss D: 0.6799, loss G: 1.8990 D(x): 0.8547\n",
            "Epoch [476/600] Batch 0/13                   Loss D: 0.6912, loss G: 2.9654 D(x): 0.9318\n",
            "Epoch [477/600] Batch 0/13                   Loss D: 0.6973, loss G: 2.9070 D(x): 0.9388\n",
            "Epoch [478/600] Batch 0/13                   Loss D: 0.6777, loss G: 2.8135 D(x): 0.9274\n",
            "Epoch [479/600] Batch 0/13                   Loss D: 0.6898, loss G: 1.9716 D(x): 0.8411\n",
            "Epoch [480/600] Batch 0/13                   Loss D: 0.6960, loss G: 2.9503 D(x): 0.9228\n",
            "Epoch [481/600] Batch 0/13                   Loss D: 0.6662, loss G: 2.4673 D(x): 0.8845\n",
            "Epoch [482/600] Batch 0/13                   Loss D: 0.6708, loss G: 2.4289 D(x): 0.9033\n",
            "Epoch [483/600] Batch 0/13                   Loss D: 0.6785, loss G: 2.8274 D(x): 0.9162\n",
            "Epoch [484/600] Batch 0/13                   Loss D: 0.6697, loss G: 2.6411 D(x): 0.9126\n",
            "Epoch [485/600] Batch 0/13                   Loss D: 0.6954, loss G: 2.9208 D(x): 0.9263\n",
            "Epoch [486/600] Batch 0/13                   Loss D: 0.6853, loss G: 2.2045 D(x): 0.8571\n",
            "Epoch [487/600] Batch 0/13                   Loss D: 0.6762, loss G: 2.1793 D(x): 0.8626\n",
            "Epoch [488/600] Batch 0/13                   Loss D: 0.6784, loss G: 2.3728 D(x): 0.8861\n",
            "Epoch [489/600] Batch 0/13                   Loss D: 0.6716, loss G: 2.4229 D(x): 0.8938\n",
            "Epoch [490/600] Batch 0/13                   Loss D: 0.6830, loss G: 2.6593 D(x): 0.9197\n",
            "Epoch [491/600] Batch 0/13                   Loss D: 0.6716, loss G: 2.1851 D(x): 0.8725\n",
            "Epoch [492/600] Batch 0/13                   Loss D: 0.6753, loss G: 2.1316 D(x): 0.8667\n",
            "Epoch [493/600] Batch 0/13                   Loss D: 0.6761, loss G: 2.2874 D(x): 0.8779\n",
            "Epoch [494/600] Batch 0/13                   Loss D: 0.6976, loss G: 3.3440 D(x): 0.9357\n",
            "Epoch [495/600] Batch 0/13                   Loss D: 0.6923, loss G: 2.9495 D(x): 0.9369\n",
            "Epoch [496/600] Batch 0/13                   Loss D: 0.6759, loss G: 2.4838 D(x): 0.8960\n",
            "Epoch [497/600] Batch 0/13                   Loss D: 0.6685, loss G: 2.2014 D(x): 0.8747\n",
            "Epoch [498/600] Batch 0/13                   Loss D: 0.6755, loss G: 2.6965 D(x): 0.9082\n",
            "Epoch [499/600] Batch 0/13                   Loss D: 0.6713, loss G: 2.2926 D(x): 0.8864\n",
            "Epoch [500/600] Batch 0/13                   Loss D: 0.6745, loss G: 2.4978 D(x): 0.8971\n",
            "Epoch [501/600] Batch 0/13                   Loss D: 0.6675, loss G: 2.5637 D(x): 0.8955\n",
            "Epoch [502/600] Batch 0/13                   Loss D: 0.6681, loss G: 2.4391 D(x): 0.8907\n",
            "Epoch [503/600] Batch 0/13                   Loss D: 0.6720, loss G: 2.7893 D(x): 0.9048\n",
            "Epoch [504/600] Batch 0/13                   Loss D: 0.7431, loss G: 3.6582 D(x): 0.9420\n",
            "Epoch [505/600] Batch 0/13                   Loss D: 0.6841, loss G: 2.7178 D(x): 0.9149\n",
            "Epoch [506/600] Batch 0/13                   Loss D: 0.6804, loss G: 2.0966 D(x): 0.8631\n",
            "Epoch [507/600] Batch 0/13                   Loss D: 0.6748, loss G: 2.1620 D(x): 0.8728\n",
            "Epoch [508/600] Batch 0/13                   Loss D: 0.6823, loss G: 2.8551 D(x): 0.9095\n",
            "Epoch [509/600] Batch 0/13                   Loss D: 0.6735, loss G: 2.5015 D(x): 0.8944\n",
            "Epoch [510/600] Batch 0/13                   Loss D: 0.6764, loss G: 2.0259 D(x): 0.8785\n",
            "Epoch [511/600] Batch 0/13                   Loss D: 0.6771, loss G: 2.3843 D(x): 0.8781\n",
            "Epoch [512/600] Batch 0/13                   Loss D: 0.6786, loss G: 2.5611 D(x): 0.8803\n",
            "Epoch [513/600] Batch 0/13                   Loss D: 0.7262, loss G: 3.4155 D(x): 0.9335\n",
            "Epoch [514/600] Batch 0/13                   Loss D: 0.6798, loss G: 2.7087 D(x): 0.9185\n",
            "Epoch [515/600] Batch 0/13                   Loss D: 0.6813, loss G: 2.5470 D(x): 0.9048\n",
            "Epoch [516/600] Batch 0/13                   Loss D: 0.6732, loss G: 2.3319 D(x): 0.8856\n",
            "Epoch [517/600] Batch 0/13                   Loss D: 0.6840, loss G: 2.9773 D(x): 0.9092\n",
            "Epoch [518/600] Batch 0/13                   Loss D: 0.6718, loss G: 2.1036 D(x): 0.8775\n",
            "Epoch [519/600] Batch 0/13                   Loss D: 0.6845, loss G: 2.9137 D(x): 0.9014\n",
            "Epoch [520/600] Batch 0/13                   Loss D: 0.6912, loss G: 3.0269 D(x): 0.9339\n",
            "Epoch [521/600] Batch 0/13                   Loss D: 0.6846, loss G: 2.9105 D(x): 0.9269\n",
            "Epoch [522/600] Batch 0/13                   Loss D: 0.6686, loss G: 2.5234 D(x): 0.8981\n",
            "Epoch [523/600] Batch 0/13                   Loss D: 0.6682, loss G: 2.3984 D(x): 0.8830\n",
            "Epoch [524/600] Batch 0/13                   Loss D: 0.6705, loss G: 2.5362 D(x): 0.9038\n",
            "Epoch [525/600] Batch 0/13                   Loss D: 0.6698, loss G: 2.3677 D(x): 0.8846\n",
            "Epoch [526/600] Batch 0/13                   Loss D: 0.6733, loss G: 2.1738 D(x): 0.8822\n",
            "Epoch [527/600] Batch 0/13                   Loss D: 0.6996, loss G: 1.9152 D(x): 0.8296\n",
            "Epoch [528/600] Batch 0/13                   Loss D: 0.6963, loss G: 2.1002 D(x): 0.8443\n",
            "Epoch [529/600] Batch 0/13                   Loss D: 0.6746, loss G: 2.1934 D(x): 0.8664\n",
            "Epoch [530/600] Batch 0/13                   Loss D: 0.6646, loss G: 2.2756 D(x): 0.8937\n",
            "Epoch [531/600] Batch 0/13                   Loss D: 0.6795, loss G: 2.4968 D(x): 0.8799\n",
            "Epoch [532/600] Batch 0/13                   Loss D: 0.6788, loss G: 2.8219 D(x): 0.9058\n",
            "Epoch [533/600] Batch 0/13                   Loss D: 0.6770, loss G: 2.1832 D(x): 0.8709\n",
            "Epoch [534/600] Batch 0/13                   Loss D: 0.6773, loss G: 2.3227 D(x): 0.8679\n",
            "Epoch [535/600] Batch 0/13                   Loss D: 0.6846, loss G: 2.1810 D(x): 0.8597\n",
            "Epoch [536/600] Batch 0/13                   Loss D: 0.6709, loss G: 2.5509 D(x): 0.9023\n",
            "Epoch [537/600] Batch 0/13                   Loss D: 0.6709, loss G: 2.5891 D(x): 0.8952\n",
            "Epoch [538/600] Batch 0/13                   Loss D: 0.6698, loss G: 2.2346 D(x): 0.8799\n",
            "Epoch [539/600] Batch 0/13                   Loss D: 0.6715, loss G: 2.1482 D(x): 0.8775\n",
            "Epoch [540/600] Batch 0/13                   Loss D: 2.0566, loss G: 0.1433 D(x): 0.2520\n",
            "Epoch [541/600] Batch 0/13                   Loss D: 1.1309, loss G: 2.9197 D(x): 0.7420\n",
            "Epoch [542/600] Batch 0/13                   Loss D: 0.9355, loss G: 2.1687 D(x): 0.6642\n",
            "Epoch [543/600] Batch 0/13                   Loss D: 1.1783, loss G: 3.8626 D(x): 0.8879\n",
            "Epoch [544/600] Batch 0/13                   Loss D: 0.8320, loss G: 2.8897 D(x): 0.8532\n",
            "Epoch [545/600] Batch 0/13                   Loss D: 0.7949, loss G: 2.7861 D(x): 0.8712\n",
            "Epoch [546/600] Batch 0/13                   Loss D: 0.7725, loss G: 3.0234 D(x): 0.9093\n",
            "Epoch [547/600] Batch 0/13                   Loss D: 0.7139, loss G: 2.1966 D(x): 0.8228\n",
            "Epoch [548/600] Batch 0/13                   Loss D: 0.7022, loss G: 2.5722 D(x): 0.8885\n",
            "Epoch [549/600] Batch 0/13                   Loss D: 0.7024, loss G: 2.8557 D(x): 0.8996\n",
            "Epoch [550/600] Batch 0/13                   Loss D: 0.6805, loss G: 2.3346 D(x): 0.8699\n",
            "Epoch [551/600] Batch 0/13                   Loss D: 0.6870, loss G: 2.4710 D(x): 0.8689\n",
            "Epoch [552/600] Batch 0/13                   Loss D: 0.6928, loss G: 2.2193 D(x): 0.8381\n",
            "Epoch [553/600] Batch 0/13                   Loss D: 0.7080, loss G: 2.8920 D(x): 0.8895\n",
            "Epoch [554/600] Batch 0/13                   Loss D: 0.6902, loss G: 2.6232 D(x): 0.9075\n",
            "Epoch [555/600] Batch 0/13                   Loss D: 0.6939, loss G: 2.5529 D(x): 0.8932\n",
            "Epoch [556/600] Batch 0/13                   Loss D: 0.6956, loss G: 2.8006 D(x): 0.9237\n",
            "Epoch [557/600] Batch 0/13                   Loss D: 0.6789, loss G: 2.3355 D(x): 0.8855\n",
            "Epoch [558/600] Batch 0/13                   Loss D: 0.6792, loss G: 2.6757 D(x): 0.8976\n",
            "Epoch [559/600] Batch 0/13                   Loss D: 0.6699, loss G: 2.4245 D(x): 0.9020\n",
            "Epoch [560/600] Batch 0/13                   Loss D: 0.6733, loss G: 2.5962 D(x): 0.8942\n",
            "Epoch [561/600] Batch 0/13                   Loss D: 0.6808, loss G: 2.4400 D(x): 0.8791\n",
            "Epoch [562/600] Batch 0/13                   Loss D: 0.6849, loss G: 2.5573 D(x): 0.9081\n",
            "Epoch [563/600] Batch 0/13                   Loss D: 0.6676, loss G: 2.4205 D(x): 0.8964\n",
            "Epoch [564/600] Batch 0/13                   Loss D: 0.6671, loss G: 2.4194 D(x): 0.8983\n",
            "Epoch [565/600] Batch 0/13                   Loss D: 0.6695, loss G: 2.4625 D(x): 0.8844\n",
            "Epoch [566/600] Batch 0/13                   Loss D: 0.6702, loss G: 2.5217 D(x): 0.9024\n",
            "Epoch [567/600] Batch 0/13                   Loss D: 0.7076, loss G: 2.8609 D(x): 0.9336\n",
            "Epoch [568/600] Batch 0/13                   Loss D: 0.6676, loss G: 2.3501 D(x): 0.8777\n",
            "Epoch [569/600] Batch 0/13                   Loss D: 0.6693, loss G: 2.3499 D(x): 0.9089\n",
            "Epoch [570/600] Batch 0/13                   Loss D: 0.6696, loss G: 2.4997 D(x): 0.8964\n",
            "Epoch [571/600] Batch 0/13                   Loss D: 0.6722, loss G: 2.5049 D(x): 0.8908\n",
            "Epoch [572/600] Batch 0/13                   Loss D: 0.6702, loss G: 2.2913 D(x): 0.8806\n",
            "Epoch [573/600] Batch 0/13                   Loss D: 0.6631, loss G: 2.5164 D(x): 0.8933\n",
            "Epoch [574/600] Batch 0/13                   Loss D: 0.6910, loss G: 2.7405 D(x): 0.9170\n",
            "Epoch [575/600] Batch 0/13                   Loss D: 0.6770, loss G: 2.0611 D(x): 0.8561\n",
            "Epoch [576/600] Batch 0/13                   Loss D: 0.6713, loss G: 2.2184 D(x): 0.8790\n",
            "Epoch [577/600] Batch 0/13                   Loss D: 0.6763, loss G: 2.3209 D(x): 0.8963\n",
            "Epoch [578/600] Batch 0/13                   Loss D: 0.6714, loss G: 2.3924 D(x): 0.8926\n",
            "Epoch [579/600] Batch 0/13                   Loss D: 0.6718, loss G: 2.4995 D(x): 0.8911\n",
            "Epoch [580/600] Batch 0/13                   Loss D: 0.6672, loss G: 2.4847 D(x): 0.8979\n",
            "Epoch [581/600] Batch 0/13                   Loss D: 0.6848, loss G: 2.8545 D(x): 0.9220\n",
            "Epoch [582/600] Batch 0/13                   Loss D: 0.6661, loss G: 2.4894 D(x): 0.9051\n",
            "Epoch [583/600] Batch 0/13                   Loss D: 0.6695, loss G: 2.5799 D(x): 0.9119\n",
            "Epoch [584/600] Batch 0/13                   Loss D: 0.6860, loss G: 2.9299 D(x): 0.9219\n",
            "Epoch [585/600] Batch 0/13                   Loss D: 0.6924, loss G: 2.7419 D(x): 0.9157\n",
            "Epoch [586/600] Batch 0/13                   Loss D: 0.6672, loss G: 2.2085 D(x): 0.8900\n",
            "Epoch [587/600] Batch 0/13                   Loss D: 0.6764, loss G: 2.2195 D(x): 0.8664\n",
            "Epoch [588/600] Batch 0/13                   Loss D: 0.6777, loss G: 2.6911 D(x): 0.9110\n",
            "Epoch [589/600] Batch 0/13                   Loss D: 0.6783, loss G: 2.7068 D(x): 0.8976\n",
            "Epoch [590/600] Batch 0/13                   Loss D: 0.6726, loss G: 2.4558 D(x): 0.9129\n",
            "Epoch [591/600] Batch 0/13                   Loss D: 0.6804, loss G: 2.8982 D(x): 0.9077\n",
            "Epoch [592/600] Batch 0/13                   Loss D: 0.6801, loss G: 2.7608 D(x): 0.9116\n",
            "Epoch [593/600] Batch 0/13                   Loss D: 0.6661, loss G: 2.4409 D(x): 0.8995\n",
            "Epoch [594/600] Batch 0/13                   Loss D: 0.6817, loss G: 2.6380 D(x): 0.8912\n",
            "Epoch [595/600] Batch 0/13                   Loss D: 0.6682, loss G: 2.3815 D(x): 0.8854\n",
            "Epoch [596/600] Batch 0/13                   Loss D: 0.6742, loss G: 2.7849 D(x): 0.9117\n",
            "Epoch [597/600] Batch 0/13                   Loss D: 0.6748, loss G: 2.4209 D(x): 0.8806\n",
            "Epoch [598/600] Batch 0/13                   Loss D: 0.6672, loss G: 2.5650 D(x): 0.9055\n",
            "Epoch [599/600] Batch 0/13                   Loss D: 0.6682, loss G: 2.4312 D(x): 0.8964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmqrWUgSNgwx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        },
        "outputId": "11f60ca2-f5fd-4c21-ccee-bd5db55820b6"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/runs/GAN_MNIST                                                                                                        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 666), started 1:59:23 ago. (Use '!kill 666' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_t_7NQE2ilW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}